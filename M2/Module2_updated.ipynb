{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870cde38",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MATH 4/5388: Machine Learning Methods \n",
    "\n",
    "\n",
    "### Module 2: Parametric Regression Models \n",
    "\n",
    "#### Instructor: Farhad Pourkamali \n",
    "\n",
    "<img src=\"CUDenverLogo.png\" width=500 height=400   align=\"right\">\n",
    "\n",
    "\n",
    "#### Spring 2023\n",
    "\n",
    "Textbook: Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow (Chapter 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72abe19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of Module 2\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Linear regression: Problem formulation, assumption, loss function, gradient\n",
    "    * Normal equation\n",
    "    * Sklearn implementation\n",
    "    * Evaluation metrics\n",
    "    * Gradient descent (GD) and variants\n",
    "    * Nonlinear extension and regularization \n",
    "    * Model selection: The process of selecting the proper level of flexibility "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404d5d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case study: univariate linear regression\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "   \n",
    "* Training data: $\\mathcal{D}=\\{(x_n,y_n)\\}_{n=1}^N$\n",
    "\n",
    "* Parametric model: $f(x)=\\theta_0+\\theta_1 x$\n",
    "\n",
    "* Objective: Choose $\\theta_0,\\theta_1$ such that $f(x_n)$ is close to $y_n$ \n",
    "\n",
    "* Mean squared error (MSE): \n",
    "\n",
    "$$\\mathcal{L}(\\theta_0, \\theta_1)=\\frac{1}{N}\\sum_{n=1}^N \\big(y_n - f(x_n)\\big)^2$$\n",
    "\n",
    "\n",
    "<center><img src=\"uni.png\" width=800 height=700><center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876d785",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving the optimization problem\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* We'll need the concept of partial derivatives\n",
    "\n",
    "* To compute $\\partial \\mathcal{L}/\\partial \\theta_0$, take the  derivative with respect to $\\theta_0$, treating the rest of the arguments as constants\n",
    "\n",
    "* We can show that \n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_0}=\\frac{-2}{N}\\sum_{n=1}^N\\big(y_n-\\theta_0-\\theta_1x_n\\big)=\\frac{2}{N}\\sum_{n=1}^N\\big(f(x_n)- y_n\\big)$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_1}=\\frac{-2}{N}\\sum_{n=1}^N\\big(y_n-\\theta_0-\\theta_1x_n\\big)x_n=\\frac{2}{N}\\sum_{n=1}^N\\big(f(x_n)- y_n\\big)x_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a663d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient \n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "* Extend the notion of derivatives to handle vector-argument functions\n",
    "    + Given $\\mathcal{L}:\\mathbb{R}^d\\mapsto \\mathbb{R}$, where $d$ is the number of input variables\n",
    "    \n",
    "    $$\\nabla \\mathcal{L}=\\begin{bmatrix}\\frac{\\partial \\mathcal{L}}{\\partial \\theta_0}\\\\ \\vdots\\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{d-1}} \\end{bmatrix}\\in\\mathbb{R}^d$$\n",
    "    \n",
    "* Example from the previous slide ($d=2$): \n",
    "\n",
    "$$\\nabla \\mathcal{L}=\\frac{2}{N}\\begin{bmatrix}\\sum_{n=1}^N\\big(f(x_n) - y_n\\big)\\\\ \\sum_{n=1}^N\\big(f(x_n) - y_n\\big)x_n \\end{bmatrix}\\in\\mathbb{R}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce5808",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compact form of gradient \n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Let us define  \n",
    "\n",
    "$$\\mathbf{X}=\\begin{bmatrix} 1 & x_1\\\\ \\vdots & \\vdots \\\\ 1 & x_N \\end{bmatrix}\\in\\mathbb{R}^{N\\times 2}, \\boldsymbol{\\theta}=\\begin{bmatrix}\\theta_0 \\\\ \\theta_1\\end{bmatrix}\\in\\mathbb{R}^{2}, \\mathbf{y}=\\begin{bmatrix}y_1 \\\\ \\vdots \\\\ y_N\\end{bmatrix}\\in\\mathbb{R}^{N}$$\n",
    "\n",
    "* Hence, we get \n",
    "\n",
    "$$\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}=\\begin{bmatrix}f(x_1) - y_1 \\\\ \\vdots \\\\ f(x_N) - y_N\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f303f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compact form of gradient \n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* The last step is to show that $\\nabla \\mathcal{L}$ can be written as \n",
    "\n",
    "$$\\frac{2}{N}\\mathbf{X}^T\\big(\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\big)=\\frac{2}{N}\\begin{bmatrix} 1 & \\ldots & 1\\\\x_1 & \\ldots & x_N\\end{bmatrix}\\begin{bmatrix}f(x_1) - y_1 \\\\ \\vdots \\\\ f(x_N) - y_N\\end{bmatrix}=\\frac{2}{N}\\begin{bmatrix}\\sum_{n=1}^N\\big(f(x_n) - y_n\\big)\\\\ \\sum_{n=1}^N\\big(f(x_n) - y_n\\big)x_n \\end{bmatrix}$$\n",
    "\n",
    "* Given this compact form, we can use NumPy to solve the linear matrix equation\n",
    "\n",
    "$$\\underbrace{\\mathbf{X}^T\\mathbf{X}}_{a}\\boldsymbol{\\theta}=\\underbrace{\\mathbf{X}^T\\mathbf{y}}_{b}$$\n",
    "\n",
    "<center><img src=\"lstsq.png\" width=700 height=600><center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494eb47",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# GDP data \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "df = pd.read_csv(\"https://github.com/ageron/data/raw/main/lifesat/lifesat.csv\")\n",
    "                 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fbfb63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = df['GDP per capita (USD)'].to_numpy()\n",
    "y = df['Life satisfaction'].to_numpy()\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bf57c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# add the column of all 1's \n",
    "\n",
    "def add_column(X):\n",
    "    '''\n",
    "    add the column of all 1's \n",
    "    '''\n",
    "    return np.concatenate(( np.ones((X.shape[0],1)), X.reshape(-1,1)), axis=1)\n",
    "\n",
    "Xcon = add_column(X)\n",
    "\n",
    "Xcon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3c3040",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# solve the problem \n",
    "\n",
    "a = np.matmul(np.transpose(Xcon), Xcon)\n",
    "\n",
    "b = np.matmul(np.transpose(Xcon), y)\n",
    "\n",
    "theta = np.linalg.lstsq(a, b, rcond=None)[0] # Cut-off ratio for small singular values\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d6bc3d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# plot the prediction model f\n",
    "\n",
    "def f(X, theta):\n",
    "    return np.matmul(X, theta)\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, \"figure.figsize\": (6,3)})\n",
    "plt.scatter(X, y, s=20, label='training data')\n",
    "\n",
    "X_test = np.array([25000, 60000])\n",
    "y_test = f(add_column(X_test), theta)\n",
    "plt.plot(X_test, y_test, 'r--', label='prediction')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('GDP')\n",
    "plt.ylabel('life satisfaction')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1959f0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear models for regression\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Given the training data set $\\mathcal{D}=\\{(\\mathbf{x}_n,y_n)\\}_{n=1}^N$ and an input vector $\\mathbf{x}\\in\\mathbb{R}^D$, the linear regression model takes the form\n",
    "\n",
    "$$f(\\mathbf{x})=\\theta_0+\\theta_1x_1+\\theta_2x_2+\\ldots+\\theta_Dx_D=\\theta_0+\\boldsymbol{\\theta}^T\\mathbf{x}$$\n",
    "    \n",
    "* $\\boldsymbol{\\theta}\\in\\mathbb{R}^D$: weights or regression coefficients, $\\theta_0$: intercept or bias term\n",
    "\n",
    "* Compact representation by defining $\\mathbf{x}=[\\color{red}{x_0=1},x_1,\\ldots,x_D]$ and $\\boldsymbol{\\theta}=[\\theta_0,\\theta_1,\\ldots,\\theta_D]$ in $\\mathbb{R}^{D+1}$\n",
    "\n",
    "$$f(\\mathbf{x})=\\boldsymbol{\\theta}^T\\mathbf{x}=\\langle \\boldsymbol{\\theta}, \\mathbf{x}\\rangle$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea5b128",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss function for linear regression\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* MSE loss function for a linear regression model\n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol{\\theta})=\\frac{1}{N}\\sum_{n=1}^N\\big(y_n-\\langle \\boldsymbol{\\theta},\\mathbf{x}_n\\rangle\\big)^2=\\frac{1}{N}\\|\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\|_2^2$$\n",
    "\n",
    "where we have\n",
    "\n",
    "$$\\mathbf{X}=\\begin{bmatrix}\\rule[.5ex]{1em}{0.4pt}\\mathbf{x}_1^T \\rule[.5ex]{1em}{0.4pt}\\\\ \\vdots\\\\\\rule[.5ex]{1em}{0.4pt}\\mathbf{x}_N^T \\rule[.5ex]{1em}{0.4pt}\\end{bmatrix}\\in\\mathbb{R}^{N\\times (D+1)}, \\boldsymbol{\\theta}=\\begin{bmatrix}\n",
    "\\theta_0\\\\\\theta_1\\\\ \\vdots \\\\\\theta_{D}\\end{bmatrix}\\in\\mathbb{R}^{D+1},\\;\\mathbf{y}=\\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots\\\\y_N\\end{bmatrix}\\in\\mathbb{R}^N, $$\n",
    "\n",
    "* Optimization problem for model fitting/training: $\\underset{\\boldsymbol{\\theta}\\in\\mathbb{R}^{D+1}}{\\operatorname{argmin}} \\mathcal{L}(\\boldsymbol{\\theta})$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2235df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Normal equation\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* To find the value of $\\boldsymbol{\\theta}$ that minimizes the MSE, there exists a *closed-form* solution\n",
    "    * a mathematical equation that gives the result directly\n",
    "    \n",
    "* The gradient takes the form \n",
    "\n",
    "$$\\nabla \\mathcal{L}(\\boldsymbol{\\theta})= \\frac{2}{N}\\mathbf{X}^T\\big(\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\big)$$\n",
    "\n",
    "* Normal equation\n",
    "\n",
    "$$\\boldsymbol{\\theta}^*=\\big(\\mathbf{X}^T\\mathbf{X}\\big)^{-1}\\mathbf{X}^T\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400bda32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sklearn implementation\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Documentation page: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html \n",
    "\n",
    "    * Parameters: Useful for creating objects \n",
    "    \n",
    "    * Attributes: Estimated coefficients, etc.\n",
    "    \n",
    "    * Methods: Training, prediction, etc.\n",
    "    \n",
    "<center><img src=\"linearreg.png\" width=900 height=800><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f5e4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Revisit the GDP data \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X.reshape(-1,1), y) # X should be a 2D array \n",
    "\n",
    "print(reg.intercept_, reg.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421732b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing data\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Change raw/original feature vectors into a representation that is more suitable for the downstream estimators/tasks\n",
    "\n",
    "* The *sklearn.preprocessing* package provides several common utility functions\n",
    "    * Scaling features to lie between a given minimum and maximum value\n",
    "    \n",
    "    * Removing the mean value and dividing features by their standard deviation\n",
    "        * features look like standard normally distributed data\n",
    "    \n",
    "* We use the first technique in the next slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a58c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Revisit the GDP data \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "X_minmax = minmax.fit_transform(X.reshape(-1,1))\n",
    "\n",
    "reg = LinearRegression()\n",
    "\n",
    "reg.fit(X_minmax, y) # X should be a 2D array \n",
    "\n",
    "print(reg.intercept_, reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3493e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbcf10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9bf0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the prediction model \n",
    "\n",
    "plt.rcParams.update({'font.size': 12, \"figure.figsize\": (6,3)})\n",
    "plt.scatter(X_minmax, y, s=20, label='training data')\n",
    "\n",
    "X_test = np.array([25000, 60000]).reshape(-1,1)\n",
    "X_test_minmax = minmax.transform(X_test)\n",
    "plt.plot(X_test_minmax, reg.predict(X_test_minmax), 'r--', label='prediction')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('GDP (scaled)')\n",
    "plt.ylabel('life satisfaction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c7332",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluation metrics for regression problems\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* The quality of a regression model can be assessed using various quantities\n",
    "    * See https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "* Mean squared error \n",
    "$$\\text{MSE}(\\mathbf{y}, \\hat{\\mathbf{y}})=\\frac{1}{N_{\\text{test}}}\\sum_{n=1}^{N_{\\text{test}}} \\big(y_n - \\hat{y}_n\\big)^2$$\n",
    "\n",
    "    * The value you get after calculating MSE is a squared unit of output\n",
    "    \n",
    "    * If you have outliers in the data set, then it penalizes the outliers most "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456594d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Root Mean squared error (RMSE)\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* The output value you get is in the same unit as the required output variable\n",
    "\n",
    "$$\\text{RMSE}(\\mathbf{y}, \\hat{\\mathbf{y}})=\\sqrt{\\frac{1}{N_{\\text{test}}}\\sum_{n=1}^{N_{\\text{test}}} \\big(y_n - \\hat{y}_n\\big)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7257fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [3, -1, 2, 7]\n",
    "y_pred = [3, 0, 2, 7]\n",
    "\n",
    "# If True returns MSE value, if False returns RMSE value.\n",
    "print('MSE: ', mean_squared_error(y_true, y_pred), \n",
    "      ', RMSE: ', mean_squared_error(y_true, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59236948",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### R² score or the coefficient of determination\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Definition \n",
    "\n",
    "$$R^2(\\mathbf{y}, \\hat{\\mathbf{y}}) = 1 - \\frac{\\sum_{n=1}^{N_{\\text{test}}} (y_n - \\hat{y}_n)^2}{\\sum_{n=1}^{N_{\\text{test}}} (y_n - \\bar{y})^2},\\;\\;\\bar{y} = \\frac{1}{N_{\\text{test}}} \\sum_{n=1}^{N_{\\text{test}}} y_n$$\n",
    "\n",
    "* RSS (Residual Sum of Squares) measures the amount of variability that is left unexplained \n",
    "\n",
    "$$\\text{RSS}=\\sum_{n=1}^{N_{\\text{test}}} (y_n - \\hat{y}_n)^2$$\n",
    "\n",
    "* Best possible score is 1 and a number near 0 indicates the model does not explain much of the variability in the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ceef05",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explained variance score\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Definition\n",
    "\n",
    "$$explained\\_{}variance(\\mathbf{y}, \\hat{\\mathbf{y}}) = 1 - \\frac{Var\\{ \\mathbf{y} - \\hat{\\mathbf{y}}\\}}{Var\\{\\mathbf{y}\\}}$$\n",
    "\n",
    "* The best possible score is 1.0, lower values are worse\n",
    "\n",
    "* When the prediction residuals have zero mean, the $R^2$ score and the Explained variance score are identical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c4ec6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, explained_variance_score\n",
    "y_true = [3, -1, 2, 7]\n",
    "y_pred = [2.9, 0, 2.5, 6.5]\n",
    "r2_score(y_true, y_pred), explained_variance_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec2e8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent (GD)\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Tweak parameters $\\boldsymbol{\\theta}$ iteratively to minimize the loss function\n",
    "$\\mathcal{L}(\\boldsymbol{\\theta})$\n",
    "\n",
    "\n",
    "* At each iteration $t$, perform an update to decrease the loss function\n",
    "\n",
    "$$\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t - \\eta_t \\nabla \\mathcal{L}(\\boldsymbol{\\theta}_t)$$\n",
    "\n",
    "where $\\eta_t$ is the step size or learning rate\n",
    "\n",
    "    \n",
    "<center><img src=\"gd.png\" width=600 height=400 ><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b3055",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning rate hyperparameter\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* A hyperparameter is a parameter that is set *before* the learning process begins\n",
    "    * These parameters are tunable and directly affect how well a model trains\n",
    "    \n",
    "* If the learning rate is too small, then the algorithm will have to go through many iterations to converge\n",
    "* The algorithm may diverge when the learning rate is too high \n",
    "\n",
    "<center><img src=\"learning_rate.png\" width=800 height=600 ><center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16359764",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One-Dimensional Gradient Descent\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Consider some continuously differentiable real-valued function $f: \\mathbb{R} \\rightarrow \\mathbb{R}$.  Using a Taylor expansion we obtain\n",
    "\n",
    "$$f(x + \\epsilon) = f(x) + \\epsilon f'(x) + \\mathcal{O}(\\epsilon^2)$$\n",
    "\n",
    "* Choose $\\epsilon = -\\eta f'(x)$ for $\\eta>0$, we get\n",
    "\n",
    "$$f(x - \\eta f'(x)) = f(x) - \\eta f'^2(x) + \\mathcal{O}(\\eta^2 f'^2(x))$$\n",
    "\n",
    "* choose $\\eta$ small enough for the higher-order terms to become irrelevant\n",
    "\n",
    "$$f(x - \\eta f'(x)) \\lessapprox f(x)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c352c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):  # Objective function\n",
    "    return x ** 2\n",
    "\n",
    "def f_grad(x):  # Gradient (derivative) of the objective function\n",
    "    return 2 * x\n",
    "\n",
    "def gd(eta, f_grad):\n",
    "    x = 10.0\n",
    "    results = [x]\n",
    "    for i in range(10):\n",
    "        x -= eta * f_grad(x)\n",
    "        results.append(float(x))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e1a14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def show_trace(results, f):\n",
    "    \n",
    "    n = max(abs(min(results)), abs(max(results)))\n",
    "    f_line = np.arange(-n, n, 0.1)\n",
    "    plt.plot(f_line, [f(x) for x in f_line], 'r-')\n",
    "    \n",
    "    for i in range(0, len(results), 1):\n",
    "        plt.plot(results[i:i+2], [f(x) for x in results[i:i+2]], 'bo-')\n",
    "    \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef61f1bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "show_trace(gd(1, f_grad), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2801dc3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Batch GD for linear regression\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Recall the gradient vector of the loss function\n",
    "\n",
    "$$\\nabla \\mathcal{L}(\\boldsymbol{\\theta})= \\frac{2}{N}\\mathbf{X}^T\\big(\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}\\big)$$\n",
    "\n",
    "* GD step with fixed learning rate\n",
    "$$\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t - \\eta \\nabla \\mathcal{L}(\\boldsymbol{\\theta}_t)=\\boldsymbol{\\theta}_t - \\eta \\frac{2}{N}\\mathbf{X}^T\\big(\\mathbf{X}\\boldsymbol{\\theta}_t - \\mathbf{y}\\big) $$\n",
    "\n",
    "    * This formula involves calculations over the full training set $\\mathbf{X}$ --> batch or full GD\n",
    "    * An epoch means one complete pass of the training data set \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926196ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# GDP data \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "df = pd.read_csv(\"https://github.com/ageron/data/raw/main/lifesat/lifesat.csv\")\n",
    "\n",
    "X = df['GDP per capita (USD)'].to_numpy()\n",
    "y = df['Life satisfaction'].to_numpy()\n",
    "\n",
    "def add_column(X):\n",
    "    '''\n",
    "    add the column of all 1's \n",
    "    '''\n",
    "    return np.concatenate(( np.ones((X.shape[0],1)), X.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57807977",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "X_minmax = minmax.fit_transform(X.reshape(-1,1))\n",
    "\n",
    "Xcon = add_column(X_minmax)\n",
    "\n",
    "print(Xcon[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c261a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Implementation of Batch GD \n",
    "\n",
    "eta = 0.01  # learning rate\n",
    "n_epochs = 1000\n",
    "N = len(Xcon)  # number of instances\n",
    "\n",
    "np.random.seed(3)\n",
    "theta = np.random.randn(2, 1)  # randomly initialized model parameters\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2 / N * Xcon.T @ (Xcon @ theta - y.reshape(-1,1))\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c548b74",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# impact of epoch and eta \n",
    "np.random.seed(3)\n",
    "theta = np.random.randn(2, 1)  # randomly initialized model parameters\n",
    "\n",
    "X_test = np.array([25000, 60000]).reshape(-1,1)\n",
    "X_test_minmax = add_column(minmax.transform(X_test))\n",
    "plt.scatter(X_minmax, y, s=20, label='training data')\n",
    "eta= .1 # 0.001, 0.01, 0.1, 1\n",
    "for epoch in range(n_epochs):\n",
    "    gradients = 2 / N * Xcon.T @ (Xcon @ theta - y.reshape(-1,1))\n",
    "    theta = theta - eta * gradients\n",
    "    if epoch == 1: \n",
    "        plt.plot(X_test_minmax[:,1], X_test_minmax@theta , 'b:', label='1 epochs')\n",
    "    elif epoch == 10:\n",
    "        plt.plot(X_test_minmax[:,1], X_test_minmax@theta , 'r--', label='10 epochs')\n",
    "    elif epoch == 100:\n",
    "        plt.plot(X_test_minmax[:,1], X_test_minmax@theta , 'g-', label='100 epochs')           \n",
    "plt.legend()\n",
    "plt.xlabel('GDP (scaled)')\n",
    "plt.ylabel('life satisfaction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27b3a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic gradient descent (SGD) for linear regression\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* The main problem with GD is that it uses the whole training set at every step \n",
    "* Consider a minibatch of size $B=1$ and a selected sample $\\mathbf{x}_n^T$ from $\\mathbf{X}$ (row vector)\n",
    "\n",
    "$$\\nabla \\mathcal{L}(\\boldsymbol{\\theta})=\\frac{2}{N}\\mathbf{X}^T(\\mathbf{X}\\boldsymbol{\\mathbf{w}}-\\mathbf{y}) \\Rightarrow 2\\mathbf{x}_n(\\mathbf{x}_n^T\\boldsymbol{\\theta}-y_n)$$\n",
    "\n",
    "$$\\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t - 2\\mathbf{x}_n(\\mathbf{x}_n^T\\boldsymbol{\\theta}-y_n)$$\n",
    "\n",
    "* Given that $N$ is the sample size and $B$ is the batch size, in one epoch we update our model $N/B$ times\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409cae8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "np.random.seed(42)\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(N):\n",
    "        random_index = np.random.randint(N)\n",
    "        xi = np.transpose(Xcon[random_index : random_index + 1])\n",
    "        yi = y[random_index : random_index + 1]\n",
    "        gradients = 2 * xi @ (xi.T @ theta - yi)  # for SGD, do not divide by N\n",
    "        eta = learning_schedule(epoch * N + iteration)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6211a80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sklearn implementation of SGD for linear regression\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "\n",
    "* Parameters\n",
    "    * max_iter: epochs\n",
    "    * learning_rate: constant or variable\n",
    "    * n_iter_no_change: number of iterations with no improvement to wait before stopping fitting\n",
    "    \n",
    "<center><img src=\"SGDRegressor.png\" width=800 height=600 ><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273507fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# GDP data \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X = df['GDP per capita (USD)'].to_numpy().reshape(-1,1)\n",
    "y = df['Life satisfaction'].to_numpy()\n",
    "\n",
    "pipe = Pipeline([('preprocess', MinMaxScaler()), \n",
    "                 ('reg', SGDRegressor(random_state=42))])\n",
    "\n",
    "pipe.fit(X, y)\n",
    "\n",
    "print(pipe['reg'].intercept_, pipe['reg'].coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d4b34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Polynomial regression \n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* The linear model may not be a good fit for many problems \n",
    "    * We can improve the fit by using a polynomial regression model of degree $d$\n",
    "\n",
    "$$f(x)=\\boldsymbol{\\theta}^T\\phi(x)$$\n",
    "\n",
    "where $\\phi(x)=[1,x,x^2,\\ldots,x^d]$\n",
    "\n",
    "* This is a simple example of feature preprocessing/engineering\n",
    "    * Benefit: linear function of parameters but nonlinear wrt input features\n",
    "\n",
    "* We can use [sklearn.preprocessing.PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to generate polynomial features\n",
    "    * Use pipeline in sklearn to assemble several steps (preprocessing + estimator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a2ad0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate simulated data \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams.update({'font.size': 16, \"figure.figsize\": (6,4)})\n",
    "\n",
    "N = 400\n",
    "X = 6 * np.random.rand(N, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(N, 1)\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a370e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# train polynomial model \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pipe = Pipeline(steps=[\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('regr', LinearRegression())])\n",
    "pipe.fit(X, y) # training \n",
    "X_new = np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "y_new = pipe.predict(X_new) # prediction\n",
    "\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38283d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Compare varying complexity levels \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for style, width, degree in ((\"g-\", 2, 30), (\"b--\", 1, 2), (\"r-+\", 1, 1)):\n",
    "    polybig_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    std_scaler = StandardScaler()\n",
    "    lin_reg = LinearRegression()\n",
    "    polynomial_regression = Pipeline([(\"poly_features\", polybig_features),\n",
    "                                      (\"std_scaler\", std_scaler),\n",
    "                                      (\"lin_reg\", lin_reg)])\n",
    "    polynomial_regression.fit(X, y)\n",
    "    y_newbig = polynomial_regression.predict(X_new)\n",
    "    plt.plot(X_new, y_newbig, style, label=str(degree), linewidth=width)\n",
    "plt.plot(X, y, \"b.\", linewidth=3)\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([-3, 3, -10, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea3bb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The bias-variance tradeoff\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* A model’s generalization/test error can be expressed as the sum of \n",
    "    * Bias due to wrong assumptions\n",
    "        * A high-bias model is likely to underfit the data\n",
    "    \n",
    "    * Variance due to excessive sensitivity to small variations in the training data\n",
    "        * A high-variance model is likely to overfit the data\n",
    "    \n",
    "    * Irreducible error due to noisiness of the data itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f7e45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The bias-variance tradeoff\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* As model flexibility increases, training error decreases, but there is a U-shape in test error\n",
    "\n",
    "* In practice, computing training error is straightforward, but estimating test error is considerably more difficult because no test data are available\n",
    "\n",
    "<center><img src=\"biasvar.png\" width=700 height=600 ><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0234644",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regularization\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "* Regularization is a way to avoid overfitting by shrinking or simplifying the model  \n",
    "    \n",
    "    $$\\mathcal{L}(\\boldsymbol{\\theta})=\\|\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\|_2^2+\\lambda C(\\boldsymbol{\\theta})$$\n",
    "    \n",
    "    \n",
    "* $\\lambda\\geq 0$ is the regularization parameter (i.e., hyperparameter) and $C(\\boldsymbol{\\theta})$ is some form of model complexity\n",
    "* We can quantify complexity using the $\\ell_2$ regularization formula, i.e., the sum of the squares of all weights\n",
    "\n",
    "$$\\ell_2 \\text{ regularization}=\\|\\boldsymbol{\\theta}\\|_2^2=\\theta_0^2+\\theta_1^2+\\theta_2^2+\\ldots$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b7b8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_1$ regularization or LASSO\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* LASSO: Least Absolute Shrinkage and Selection Operator \n",
    "    * Uses the $\\ell_1$ norm of weights, instead of $\\ell_2$\n",
    "    \n",
    "    $$\\|\\boldsymbol{\\theta}\\|_1=|\\theta_0|+|\\theta_1|+|\\theta_2|+\\ldots$$\n",
    "          \n",
    "* Definition of the $\\ell_p$ norm for a real number $p\\geq 1$\n",
    "\n",
    "$$\\|\\boldsymbol{\\theta}\\|_p=\\big(\\sum_i |\\theta_i|^p\\big)^{1/p}$$\n",
    "* LASSO tends to eliminate the weights of the least important features (i.e., set them to zero)\n",
    "    * Next slide provides an intuitive explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b07ba27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\ell_1$ vs $\\ell_2$ regularization \n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Write optimization problems in bound constrained forms\n",
    "$$\\underset{\\boldsymbol{\\theta}}{\\operatorname{min}} \\mathcal{L}(\\boldsymbol{\\theta}) \\;\\text{ s.t. }\\; \\|\\boldsymbol{\\theta}\\|_1\\leq B \\;\\text{ or }\\; \\|\\boldsymbol{\\theta}\\|_2^2\\leq B$$\n",
    "\n",
    "* Let us look at the contours of the $\\ell_1$ and $\\ell_2$ constrained surfaces\n",
    "    * Corners of the $\\ell_1$ ball are more likely to intersect the ellipse than one of the sides\n",
    "\n",
    "<center><img src=\"opt.png\" width=600 height=500 ><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a5622",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "D = 5\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "theta = np.random.uniform(size=D)\n",
    "\n",
    "theta /= np.linalg.norm(theta)\n",
    "\n",
    "print(theta, np.linalg.norm(theta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5122c37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "num_exper = np.logspace(3,6,40)\n",
    "\n",
    "result = np.zeros(len(num_exper))\n",
    "\n",
    "sigma = 2\n",
    "\n",
    "for ind in range(len(num_exper)):\n",
    "    num = int(num_exper[ind])\n",
    "    X = sigma * np.random.randn(num, D)\n",
    "    result[ind] = np.mean(np.power(np.matmul(X, theta), 2))\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, \"figure.figsize\": (5,3)})\n",
    "plt.plot(num_exper, result, 'C2-')\n",
    "plt.axhline(sigma**2, c='C4')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('number of experiments')\n",
    "plt.ylabel('averaged value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc778e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cd333",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Synthetic/simulated data \n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 20\n",
    "X = 3 * np.random.rand(N, 1)\n",
    "y = 1 + 0.5 * X + np.random.randn(N, 1) / 1.5\n",
    "X_new = np.linspace(0, 3, 100).reshape(100, 1)\n",
    "alphas=(0, 10**-5, 1000)\n",
    "for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")): # zip: aggregates them in a tuple\n",
    "    model = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "        (\"regul_reg\", Ridge(alpha=alpha)), # alpha = lambda \n",
    "                    ])\n",
    "    model.fit(X, y)\n",
    "    plt.plot(X_new, model.predict(X_new), style, linewidth=2, label=r\"$\\alpha = {}$\".format(alpha))\n",
    "plt.plot(X, y, \"k.\", markersize=10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ba4ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Lasso \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# “Hitters” with 20 variables and 322 observations of major league players\n",
    "df = pd.read_csv('Hitters.csv', index_col=[0])\n",
    "\n",
    "# drop missing cases\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63e58e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing, encode our categorical features as one-hot numeric features \n",
    "\n",
    "dummies = pd.get_dummies(df[['League', 'Division','NewLeague']])\n",
    "\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6420d7b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Find outputs \n",
    "\n",
    "y = df['Salary']\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0bced0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# drop the \"Salary\" column  and categorical columns \n",
    "\n",
    "df_input = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1)\n",
    "\n",
    "\n",
    "# Create all features\n",
    "X = pd.concat([df_input, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1).to_numpy()\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e3557",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data set into train and test set 70/30 \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10) # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d36ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba358aca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0843203b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Should we use preprocessing?\n",
    "\n",
    "* Let us try sklearn.preprocessing.StandardScaler\n",
    "\n",
    "    * Standardize features by removing the mean and scaling to unit variance\n",
    "    \n",
    "    $$z_n=\\frac{x_n - \\mu}{\\sigma}$$\n",
    "    \n",
    "    with mean \n",
    "    \n",
    "    $$\\mu=\\frac{1}{N}\\sum_{n=1}^N x_n$$\n",
    "    \n",
    "    and standard deviation \n",
    "    \n",
    "    $$\\sigma=\\sqrt{\\frac{1}{N}\\sum_{n=1}^N (x_n - \\mu)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c55a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "a = scaler.fit_transform(X_train[:,0:16])\n",
    "\n",
    "b = scaler.transform(X_test[:,0:16])\n",
    "\n",
    "X_train = np.concatenate((a, X_train[:,16:]), axis=1)\n",
    "\n",
    "X_test = np.concatenate((b, X_test[:,16:]), axis=1)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec3e5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alphas = np.arange(10, 205, 10)\n",
    "\n",
    "lasso = Lasso(max_iter=10000)\n",
    "\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    coefs.append(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02dfd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coefs[0] # 16, 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7637d80",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 15, \"figure.figsize\": (14,5)})\n",
    "max_vals = [item.max() for item in coefs]\n",
    "min_vals = [item.min() for item in coefs]\n",
    "zero_vals = [np.sum(item == 0) for item in coefs]\n",
    "plt.subplot(121)\n",
    "plt.plot(alphas, max_vals, 'b--')\n",
    "plt.plot(alphas, min_vals, 'r--')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('max/min values')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(alphas, zero_vals)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('number of zeros')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734935ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "alphas = np.arange(10, 205, 10)\n",
    "\n",
    "lasso = Lasso(max_iter=10000)\n",
    "\n",
    "r2_values = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    r2_values.append(r2_score(y_test, lasso.predict(X_test)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9095032",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.plot(alphas, r2_values, 'b--')\n",
    "plt.plot(alphas[3], r2_values[3], 'ro')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('r2 values')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(alphas, zero_vals, 'b--')\n",
    "plt.plot(alphas[3], zero_vals[3], 'ro')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('number of zeros')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbcb128",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating generalization error\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Two important tasks in machine learning \n",
    "    * having chosen a model, estimating its prediction error (generalization error) on new data -> model assessment\n",
    "    * selecting the proper level of flexibility for a model -> model selection\n",
    "    \n",
    "* Question: how can we use the *available* data to perform model selection/assessment?\n",
    "\n",
    "* We discuss two of the most commonly-used resampling methods\n",
    "    * cross-validation \n",
    "    * bootstrap\n",
    "    \n",
    "*  Key concepts remain the same regardless of whether the response\n",
    "is quantitative or qualitative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a6f88",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approach 1: Validation Set \n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* It involves *randomly* dividing the available set of observations into two parts, a training set and a validation set (or hold-out/test set)\n",
    " *  A model is fit on the training set, and its performance is evaluated on the validation set\n",
    " * If we repeat the process of randomly splitting the data, we will get different results (for example, the previous case study)\n",
    " * Only a subset of the data is used to fit the model -> tend to overestimate the generalization error\n",
    "\n",
    "<center><img src=\"approach1.png\" width=500 height=400><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9096d2be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approach 2:  Leave-One-Out Cross-Validation (LOOCV)\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Use a single observation $(x_1,y_1)$ for the validation set and $\\{(x_2,y_2),\\ldots,(x_N,y_N)\\}$ for model fitting to compute $\\text{MSE}_1=(y_1-\\hat{y}_1)^2$\n",
    "\n",
    "* Repeat this process by selecting one sample for validation and the remaining $N-1$ for training \n",
    "\n",
    "$$\\text{CV}=\\frac{1}{N}\\sum_{n=1}^N \\text{MSE}_n$$\n",
    "\n",
    "* There is no randomness in the training/validation set splits\n",
    "\n",
    "<center><img src=\"approach2.png\" width=400 height=300><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d2be58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X, y = np.arange(10).reshape((5, 2)), np.arange(5)\n",
    "print('X is: ', X, '\\n', 'y is: ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65083b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66806dc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approach 3:  K-Fold Cross-Validation\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* It involves randomly dividing the available data into $K$ groups, or folds, of approximately equal size\n",
    "    * One fold is treated as a validation set and the remaining $K-1$ folds are used for training\n",
    "    * Common choices $K=5$ or $10$\n",
    "    * Advantage over LOOCV: computational savings\n",
    "    \n",
    "<center><img src=\"approach3.png\" width=450 height=350><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46ef7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556239a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "lasso = linear_model.Lasso(alpha=0.01)\n",
    "cv_results = cross_val_score(lasso, X, y, scoring='r2', cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" \n",
    "      % (cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a385f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, \"figure.figsize\": (4,3)})\n",
    "\n",
    "plt.boxplot(cv_results, vert=False)\n",
    "plt.yticks([])\n",
    "plt.xlabel('r2 values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eaac83",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Approach 4:  The Bootstrap\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* We obtain distinct data sets by repeatedly sampling observations from the original data set *with replacement*\n",
    "    * Each of these “bootstrap data sets” is the same size as our original data\n",
    "    \n",
    "* A graphical illustration on a small data set with $N=3$\n",
    "\n",
    "<center><img src=\"approach4.png\" width=350 height=300><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8812aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model selection\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* Model selection is the process of selecting the best one by comparing and validating with various parameters\n",
    "    * https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "<center><img src=\"selection.png\" width=700 height=600 ><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd361f67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Exhaustive search over specified parameter values for an estimator.\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "parameters = {'alpha': np.linspace(1,300,100)}\n",
    "\n",
    "lasso = Lasso(max_iter=10000)\n",
    "\n",
    "reg = GridSearchCV(lasso, parameters, scoring='r2') \n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb270813",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set the best alpha\n",
    "\n",
    "lasso_best = Lasso(alpha=reg.best_params_['alpha'], max_iter=10000)\n",
    "\n",
    "lasso_best.fit(X_train, y_train)\n",
    "\n",
    "lasso_best.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b830d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# R2 for training and test sets \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pred_train = lasso_best.predict(X_train)\n",
    "r2_train = r2_score(y_train, pred_train)\n",
    "print('R2 training set', round(r2_train, 2))\n",
    "\n",
    "# Test data\n",
    "pred_test = lasso_best.predict(X_test)\n",
    "r2_test = r2_score(y_test, pred_test)\n",
    "print('R2 test set', round(r2_test, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60da40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Robust regression\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* When we have outliers, least squares doesn't always perform well!\n",
    "\n",
    "* Thus, we have to modify least squares to pick a $\\boldsymbol{\\theta}$ that minimizes the general objective function\n",
    "\n",
    "$$\\sum_{n=1}^{N} g\\big(y_n - \\boldsymbol{\\theta}^T\\mathbf{x}_n\\big)$$\n",
    "\n",
    "\n",
    "* Instead of using $g(r)=r^2$, we can use the Huber's method\n",
    "\n",
    "$$g(r)=\\begin{cases}r^2/2 & \\text{if } |r|<\\delta \\\\\n",
    "\\delta|r| - \\delta^2/2 & \\text{otherwise}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114d303",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"huber.png\" width=800 height=700 ><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab37eb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.special import huber\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "r = np.linspace(-6,6,100)\n",
    "\n",
    "deltas = [.5, 1, 2]\n",
    "linestyles = [\"dashed\", \"dotted\", \"dashdot\"]\n",
    "fig, ax = plt.subplots()\n",
    "combined_plot_parameters = list(zip(deltas, linestyles))\n",
    "for delta, style in combined_plot_parameters:\n",
    "    ax.plot(r, huber(delta, r), label=f\"$\\delta={delta}$\", ls=style)\n",
    "    \n",
    "ax.plot(r, r**2, 'r-', label='quadratic')\n",
    "ax.legend(loc=\"upper center\")\n",
    "ax.set_xlabel(\"$r$\")\n",
    "ax.set_title(\"Huber loss\")\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(0, 11)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04479ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import HuberRegressor, Ridge\n",
    "\n",
    "# Generate toy data\n",
    "rng = np.random.RandomState(0)\n",
    "X, y = make_regression(\n",
    "    n_samples=20, n_features=1, random_state=0, noise=4.0, bias=100.0\n",
    ")\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6021dd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Add four strong outliers to the dataset.\n",
    "X_outliers = rng.normal(0, 0.5, size=(4, 1))\n",
    "y_outliers = rng.normal(0, 2.0, size=4)\n",
    "\n",
    "X_outliers[:2, :] += X.max() + X.mean() / 4.0\n",
    "X_outliers[2:, :] += X.min() - X.mean() / 4.0\n",
    "y_outliers[:2] += y.min() - y.mean() / 4.0\n",
    "y_outliers[2:] += y.max() + y.mean() / 4.0\n",
    "\n",
    "X = np.vstack((X, X_outliers))\n",
    "y = np.concatenate((y, y_outliers))\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, \"figure.figsize\": (6,4)})\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237f9558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the huber regressor over a series of epsilon values.\n",
    "colors = [\"r-\", \"b-\", \"y-\", \"m-\"]\n",
    "\n",
    "plt.rcParams.update({'font.size': 12, \"figure.figsize\": (6,4)})\n",
    "x = np.linspace(X.min(), X.max(), 7)\n",
    "epsilon_values = [1, 1.5, 1.75, 1.9] # like delta \n",
    "for k, epsilon in enumerate(epsilon_values):\n",
    "    huber = HuberRegressor(alpha=0.0, epsilon=epsilon)\n",
    "    huber.fit(X, y)\n",
    "    coef_ = huber.coef_ * x + huber.intercept_\n",
    "    plt.plot(x, coef_, colors[k], label=\"huber loss, %s\" % epsilon)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.title(\"HuberRegressor\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e238fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi target regression\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "* We can use sklearn.multioutput.MultiOutputRegressor\n",
    "\n",
    "    * This strategy consists of fitting one regressor per target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134dbb41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_linnerud\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X, y = load_linnerud(return_X_y=True)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa680cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# train/test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) \n",
    "\n",
    "# train \n",
    "regr = MultiOutputRegressor(Ridge()).fit(X_train, y_train)\n",
    "\n",
    "# predict \n",
    "\n",
    "regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3bd790",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "regr.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a386a8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(3): \n",
    "    print(np.dot(regr.estimators_[i].coef_, X_test[0]) + regr.estimators_[i].intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b17b1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68eaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ded497b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# If you want to convert this notebook to pdf \n",
    "!jupyter nbconvert --to slides Module2.ipynb --post serve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8fc4d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
